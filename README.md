# Neural Network Initialization from Scratch

This repository contains code and experiments for the study:

**"Neural Network Initialization From Scratch"**  
by **Betty, Fandresena, and Vicent**

---

## Authors

- **Vicent** – [vicentmwanda](https://github.com/vicentmwanda)  
- **Fandresena** – [Efandresena](https://github.com/Efandresena)  
- **Betty** – [bettyabraham-n](https://github.com/bettyabraham-n)

---

## Overview

This project explores the impact of different weight initialization methods on neural network training.  
It compares **Random Initialization**, **Xavier Initialization**, and **Normal Xavier Initialization** using multiple network architectures and visualizes their effects on convergence and performance.

Key components include:

- Implementation of **activation functions** (Sigmoid, Tanh)  
- Forward and backward propagation for custom **neural network layers**  
- Cost computation using **cross-entropy loss**  
- Visualizations of **training/test costs**, **gradients**, and **variance evolution**  
- Comparison of different **initialization methods** across architectures

---

## Installation

1. Clone the repository:

```bash
git clone https://github.com/<your-username>/Neural-Network-Initialization-from-Scratch.git
